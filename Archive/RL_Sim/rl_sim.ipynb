{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # To deal with data in form of matrices\n",
    "import tkinter as tk # To build GUI\n",
    "import time # Time is needed to slow down the agent and to see how he runs\n",
    "from PIL import Image, ImageTk # For adding images into the canvas widget\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the sizes for the environment\n",
    "pixels = 40 # pixels/box\n",
    "env_height = 9 # grid height (row)\n",
    "env_width = 9 # grid width (column)\n",
    "# Global variable for dictionary with coordinates for the final route\n",
    "a = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the environment\n",
    "def build_environment(self):\n",
    "    self.canvas_widget = tk.Canvas(self, bg='white',height=env_height * pixels,width=env_width * pixels)\n",
    "    # Creating grid lines along x and y axis\n",
    "    for column in range(0, env_width * pixels, pixels):\n",
    "        x0, y0, x1, y1 = column, 0, column, env_height * pixels\n",
    "        self.canvas_widget.create_line(x0, y0, x1, y1, fill='grey')\n",
    "    for row in range(0, env_height * pixels, pixels):\n",
    "        x0, y0, x1, y1 = 0, row, env_height * pixels, row\n",
    "        self.canvas_widget.create_line(x0, y0, x1, y1, fill='grey')\n",
    "        \n",
    "    # Creating objects of Obstacles\n",
    "    # Obstacle type 1 - road closed1\n",
    "    img_obstacle1 = Image.open(\"road_closed1.jpeg\")\n",
    "    self.obstacle1_object = ImageTk.PhotoImage(img_obstacle1)\n",
    "    # Obstacle type 2 - tree1\n",
    "    img_obstacle2 = Image.open(\"tree1.jpeg\")\n",
    "    self.obstacle2_object = ImageTk.PhotoImage(img_obstacle2)\n",
    "    # Obstacle type 3 - tree2\n",
    "    img_obstacle3 = Image.open(\"tree2.jpeg\")\n",
    "    self.obstacle3_object = ImageTk.PhotoImage(img_obstacle3)\n",
    "    # Obstacle type 4 - building1\n",
    "    img_obstacle4 = Image.open(\"building1.jpeg\")\n",
    "    self.obstacle4_object = ImageTk.PhotoImage(img_obstacle4)\n",
    "    # Obstacle type 5 - building2\n",
    "    img_obstacle5 = Image.open(\"building2.jpeg\")\n",
    "    self.obstacle5_object = ImageTk.PhotoImage(img_obstacle5)\n",
    "    # Obstacle type 6 - road closed2\n",
    "    img_obstacle6 = Image.open(\"road_closed2.jpeg\")\n",
    "    self.obstacle6_object = ImageTk.PhotoImage(img_obstacle6)\n",
    "    # Obstacle type 7 - road closed3\n",
    "    img_obstacle7 = Image.open(\"road_closed3.jpeg\")\n",
    "    self.obstacle7_object = ImageTk.PhotoImage(img_obstacle7)\n",
    "    # Obstacle type 8 - traffic lights\n",
    "    img_obstacle8 = Image.open(\"traffic_lights.jpeg\")\n",
    "    self.obstacle8_object = ImageTk.PhotoImage(img_obstacle8)\n",
    "    # Obstacle type 9 - pedestrian\n",
    "    img_obstacle9 = Image.open(\"pedestrian.jpeg\")\n",
    "    self.obstacle9_object = ImageTk.PhotoImage(img_obstacle9)\n",
    "    # Obstacle type 10 - shop\n",
    "    img_obstacle10 = Image.open(\"shop.jpeg\")\n",
    "    self.obstacle10_object = ImageTk.PhotoImage(img_obstacle10)\n",
    "    # Obstacle type 11 - bank1\n",
    "    img_obstacle11 = Image.open(\"bank1.jpeg\")\n",
    "    self.obstacle11_object = ImageTk.PhotoImage(img_obstacle11)\n",
    "    # Obstacle type 12 - bank2\n",
    "    img_obstacle12 = Image.open(\"bank2.jpeg\")\n",
    "    self.obstacle12_object = ImageTk.PhotoImage(img_obstacle12)\n",
    "    # Creating obstacles themselves\n",
    "    # Obstacles from 1 to 22\n",
    "    self.obstacle1 = self.canvas_widget.create_image(pixels * 3, pixels * 4, anchor='nw', image=self.obstacle2_object)\n",
    "    # Obstacle 2\n",
    "    self.obstacle2 = self.canvas_widget.create_image(0, pixels * 2, anchor='nw', image=self.obstacle6_object)\n",
    "    # Obstacle 3\n",
    "    self.obstacle3 = self.canvas_widget.create_image(pixels, 0, anchor='nw', image=self.obstacle5_object)\n",
    "    # Obstacle 4\n",
    "    self.obstacle4 = self.canvas_widget.create_image(pixels * 3, pixels * 2, anchor='nw', image=self.obstacle2_object)\n",
    "    # Obstacle 5\n",
    "    self.obstacle5 = self.canvas_widget.create_image(pixels * 4, 0, anchor='nw', image=self.obstacle12_object)\n",
    "    # Obstacle 6\n",
    "    self.obstacle6 = self.canvas_widget.create_image(pixels * 5, pixels * 3, anchor='nw', image=self.obstacle7_object)\n",
    "    # Obstacle 7\n",
    "    self.obstacle7 = self.canvas_widget.create_image(pixels * 7, pixels * 3, anchor='nw', image=self.obstacle9_object)\n",
    "    # Obstacle 8\n",
    "    self.obstacle8 = self.canvas_widget.create_image(pixels * 6, pixels, anchor='nw', image=self.obstacle10_object)\n",
    "    # Obstacle 9\n",
    "    self.obstacle9 = self.canvas_widget.create_image(pixels * 5, pixels * 5, anchor='nw', image=self.obstacle4_object)\n",
    "    # Obstacle 10\n",
    "    self.obstacle10 = self.canvas_widget.create_image(pixels * 6, pixels * 5, anchor='nw', image=self.obstacle4_object)\n",
    "    # Obstacle 11\n",
    "    self.obstacle11 = self.canvas_widget.create_image(pixels * 5, pixels * 6, anchor='nw', image=self.obstacle4_object)\n",
    "    # Obstacle 12\n",
    "    self.obstacle12 = self.canvas_widget.create_image(pixels * 5, pixels * 7, anchor='nw', image=self.obstacle4_object)\n",
    "    # Obstacle 13\n",
    "    self.obstacle13 = self.canvas_widget.create_image(0, pixels * 8, anchor='nw', image=self.obstacle3_object)\n",
    "    # Obstacle 14\n",
    "    self.obstacle14 = self.canvas_widget.create_image(pixels * 3, pixels * 7, anchor='nw', image=self.obstacle8_object)\n",
    "    # Obstacle 15\n",
    "    self.obstacle15 = self.canvas_widget.create_image(0, pixels * 4, anchor='nw', image=self.obstacle1_object)\n",
    "    # Obstacle 16\n",
    "    self.obstacle16 = self.canvas_widget.create_image(pixels * 8, 0, anchor='nw', image=self.obstacle3_object)\n",
    "    # Obstacle 17\n",
    "    self.obstacle17 = self.canvas_widget.create_image(pixels * 7, pixels * 7, anchor='nw', image=self.obstacle4_object)\n",
    "    # Obstacle 18\n",
    "    self.obstacle18 = self.canvas_widget.create_image(pixels, pixels * 6, anchor='nw', image=self.obstacle11_object)\n",
    "    # Obstacle 19\n",
    "    self.obstacle19 = self.canvas_widget.create_image(pixels * 8, pixels * 3, anchor='nw', image=self.obstacle8_object)\n",
    "    # Obstacle 20\n",
    "    self.obstacle20 = self.canvas_widget.create_image(pixels * 7, pixels * 6, anchor='nw', image=self.obstacle4_object)\n",
    "    # Obstacle 21\n",
    "    self.obstacle21 = self.canvas_widget.create_image(pixels * 7, pixels * 5, anchor='nw', image=self.obstacle4_object)\n",
    "    # Obstacle 22\n",
    "    self.obstacle22 = self.canvas_widget.create_image(pixels * 2, pixels * 3, anchor='nw', image=self.obstacle2_object)\n",
    "    # Final Point\n",
    "    img_flag = Image.open(\"flag.jpeg\")\n",
    "    self.flag_object = ImageTk.PhotoImage(img_flag)\n",
    "    self.flag = self.canvas_widget.create_image(pixels * 6, pixels * 6, anchor='nw', image=self.flag_object)\n",
    "    # Uploading the image of Mobile Robot\n",
    "    img_robot = Image.open(\"agent1.jpeg\")\n",
    "    self.robot = ImageTk.PhotoImage(img_robot)\n",
    "    # Creating an agent with photo of Mobile Robot\n",
    "    self.agent = self.canvas_widget.create_image(0, 0, anchor='nw', image=self.robot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packing everything\n",
    "# Function to reset the environment and start new Episode self canvas_widget pack()\n",
    "def reset(self):\n",
    "    # Updating agent\n",
    "    self.canvas_widget.delete(self.agent)\n",
    "    self.agent = self.canvas_widget.create_image(0, 0, anchor='nw', image=self.robot)\n",
    "    # Clearing the dictionary and the i\n",
    "    self.d = {}\n",
    "    self.i = 0\n",
    "    # Return observation\n",
    "    return self.canvas_widget.coords(self.agent)\n",
    "# Function to get the next observation and reward by doing next step\n",
    "# state [1], base_action [1] = indicates row\n",
    "# state [0], , base_action [0] = indicates column\n",
    "def step(self, action):\n",
    "    # Current state of the agent\n",
    "    state = self.canvas_widget.coords(self.agent)\n",
    "    base_action = np.array([0, 0])\n",
    "    # Updating next state according to the action\n",
    "    # Action 'up'\n",
    "    if action == 0:\n",
    "        if state[1] >= pixels:\n",
    "            base_action[1] -= pixels\n",
    "    # Action 'down'\n",
    "    elif action == 1:\n",
    "        if state[1] < (env_height - 1) * pixels:\n",
    "            base_action[1] += pixels\n",
    "    # Action right\n",
    "    elif action == 2:\n",
    "        if state[0] < (env_width - 1) * pixels:\n",
    "            base_action[0] += pixels\n",
    "    # Action left\n",
    "    elif action == 3:\n",
    "        if state[0] >= pixels:\n",
    "            base_action[0] -= pixels\n",
    "    # Moving the agent according to the action\n",
    "    self.canvas_widget.move(self.agent, base_action[0], base_action[1])\n",
    "    # Writing in the dictionary coordinates of found route\n",
    "    self.d[self.i] = self.canvas_widget.coords(self.agent)\n",
    "    # Updating next state\n",
    "    next_state = self.d[self.i]\n",
    "    # Updating key for the dictionary\n",
    "    self.i += 1\n",
    "    # Calculating the reward for the agent\n",
    "    if next_state == self.canvas_widget.coords(self.flag):\n",
    "        reward = 1\n",
    "        done = True\n",
    "        next_state = 'goal'\n",
    "        # Filling the dictionary first time\n",
    "        if self.c == True:\n",
    "            for j in range(len(self.d)):\n",
    "                self.f[j] = self.d[j]\n",
    "            self.c = False\n",
    "            self.longest = len(self.d)\n",
    "            self.shortest = len(self.d)\n",
    "        # Checking if the currently found route is shorter\n",
    "        if len(self.d) < len(self.f):\n",
    "            # Saving the number of steps for the shortest route\n",
    "            self.shortest = len(self.d)\n",
    "            # Clearing the dictionary for the final route\n",
    "            self.f = {}\n",
    "            # Reassigning the dictionary\n",
    "            for j in range(len(self.d)):\n",
    "                self.f[j] = self.d[j]\n",
    "        # Saving the number of steps for the longest route\n",
    "        if len(self.d) > self.longest:\n",
    "            self.longest = len(self.d)\n",
    "    elif next_state in [self.canvas_widget.coords(self.obstacle1),\n",
    "                        self.canvas_widget.coords(self.obstacle2),\n",
    "                        self.canvas_widget.coords(self.obstacle3),\n",
    "                        self.canvas_widget.coords(self.obstacle4),\n",
    "                        self.canvas_widget.coords(self.obstacle5),\n",
    "                        self.canvas_widget.coords(self.obstacle6),\n",
    "                        self.canvas_widget.coords(self.obstacle7),\n",
    "                        self.canvas_widget.coords(self.obstacle8),\n",
    "                        self.canvas_widget.coords(self.obstacle9),\n",
    "                        self.canvas_widget.coords(self.obstacle10),\n",
    "                        self.canvas_widget.coords(self.obstacle11),\n",
    "                        self.canvas_widget.coords(self.obstacle12),\n",
    "                        self.canvas_widget.coords(self.obstacle13),\n",
    "                        self.canvas_widget.coords(self.obstacle14),\n",
    "                        self.canvas_widget.coords(self.obstacle15),\n",
    "                        self.canvas_widget.coords(self.obstacle16),\n",
    "                        self.canvas_widget.coords(self.obstacle17),\n",
    "                        self.canvas_widget.coords(self.obstacle18),\n",
    "                        self.canvas_widget.coords(self.obstacle19),\n",
    "                        self.canvas_widget.coords(self.obstacle20),\n",
    "                        self.canvas_widget.coords(self.obstacle21),\n",
    "                        self.canvas_widget.coords(self.obstacle22)]:\n",
    "        reward = -1\n",
    "        done = True\n",
    "        next_state = 'obstacle'\n",
    "        # Clearing the dictionary and the i\n",
    "        self.d = {}\n",
    "        self.i = 0\n",
    "    else:\n",
    "        reward = 0\n",
    "        done = False\n",
    "    return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to refresh the environment return next_state reward done\n",
    "def render(self):\n",
    "    self.update()\n",
    "# Function to show the found route\n",
    "def final(self):\n",
    "    # Deleting the agent at the end\n",
    "    self.canvas_widget.delete(self.agent)\n",
    "    # Showing the number of steps\n",
    "    print('The shortest route:', self.shortest)\n",
    "    print('The longest route:', self.longest)\n",
    "    # Creating initial point for robot position (oval)\n",
    "    origin = np.array([20, 20])\n",
    "    self.initial_point = self.canvas_widget.create_oval(origin[0] - 5, origin[1] - 5, origin[0] + 5, origin[1] + 5,\n",
    "    fill='blue', outline='blue')\n",
    "    # Filling the route\n",
    "    for j in range(len(self.f)):\n",
    "        # Showing the coordinates of the final route\n",
    "        print(self.f[j])\n",
    "        self.track = self.canvas_widget.create_oval(\n",
    "            self.f[j][0] + origin[0] - 5, self.f[j][1] + origin[0] - 5,\n",
    "            self.f[j][0] + origin[0] + 5, self.f[j][1] + origin[0] + 5,fill='blue', outline='blue')\n",
    "        # Writing the final route in the global variable a\n",
    "        a[j] = self.f[j]\n",
    "\n",
    "# Returning the final dictionary with route coordinates\n",
    "def final_states():\n",
    "    return a\n",
    "\n",
    "# Creating class for the environment\n",
    "class Environment(tk.Tk, object):\n",
    "    # Initializing the parameters for the environment\n",
    "    def __init__(self):\n",
    "        super(Environment, self).__init__()\n",
    "        self.action_space = ['up', 'down', 'left', 'right']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.title('Reinforcement Learning_SARSA')\n",
    "        # defining the total environment size (360 x 360 pixels)\n",
    "        self.geometry('{0}x{1}'.format(env_height * pixels, env_height * pixels))\n",
    "        self.b_env()\n",
    "        # Dictionaries to draw the final route (comparison of routes - d and f)\n",
    "        self.d = {}\n",
    "        self.f = {}\n",
    "        # Key for the dictionaries\n",
    "        self.i = 0\n",
    "        # Writing the final dictionary first time\n",
    "        self.c = True\n",
    "        # Showing the steps for longest found route\n",
    "        self.longest = 0\n",
    "        # Showing the steps for the shortest route\n",
    "        self.shortest = 0\n",
    "    b_env = build_environment\n",
    "    rst = reset\n",
    "    action = step\n",
    "    updt = render\n",
    "    dest = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3492/3344021573.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Calling for the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PF\\Miniconda\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \u001b[1;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1283\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1284\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[1;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calling for the environment\n",
    "env = Environment()\n",
    "env.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for choosing the action for the agent\n",
    "def choose_action(self, observation):\n",
    "    # Checking if the state exists in the table\n",
    "    self.ch_st_ext(observation)\n",
    "    # Selection of the action - 90 % according to the epsilon == 0.9 -> From Q-table\n",
    "    # Choosing the best action\n",
    "    if np.random.uniform() < self.epsilon:\n",
    "        state_action = self.q_table.loc[observation, :]\n",
    "        state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "        action = state_action.idxmax()\n",
    "    else:\n",
    "        # Choosing random action - left 10 % for choosing randomly\n",
    "        action = np.random.choice(self.actions)\n",
    "    return action\n",
    "# Function for learning and updating Q-table with new knowledge\n",
    "def learn(self, state, action, reward, next_state, next_action):\n",
    "    # Checking if the next step exists in the Q-table\n",
    "    self.ch_st_ext(next_state)\n",
    "    # Current state in the current position\n",
    "    q_predict = self.q_table.loc[state, action]\n",
    "    # Checking if the next state is free or it is obstacle or goal\n",
    "    if next_state != 'goal' or next_state != 'obstacle':\n",
    "        q_target = reward + self.gamma * self.q_table.loc[next_state, next_action]\n",
    "    else:\n",
    "        q_target = reward\n",
    "    # Updating Q-table with new knowledge\n",
    "    self.q_table.loc[state, action] += self.lr * (q_target - q_predict)\n",
    "    return self.q_table.loc[state, action]\n",
    "# Adding to the Q-table new states\n",
    "def check_state_exist(self, state):\n",
    "    if state not in self.q_table.index:\n",
    "        self.q_table = self.q_table.append(\n",
    "        pd.Series(\n",
    "        [0]*len(self.actions),\n",
    "        index=self.q_table.columns,\n",
    "        name=state,\n",
    "    )\n",
    "    )\n",
    "# Printing the Q-table with states\n",
    "def print_q_table(self):\n",
    "    # Getting the coordinates of final route from env.py\n",
    "    e = final_states()\n",
    "    # Comparing the indexes with coordinates and writing in the new Q-table values\n",
    "    for i in range(len(e)):\n",
    "        state = str(e[i]) # state = '[5.0, 40.0]'\n",
    "        # Going through all indexes and checking\n",
    "        for j in range(len(self.q_table.index)):\n",
    "            if self.q_table.index[j] == state:\n",
    "                self.q_table_final.loc[state, :] = self.q_table.loc[state, :]\n",
    "    print()\n",
    "    print('Length of final Q-table =', len(self.q_table_final.index))\n",
    "    print('Final Q-table with values from the final route:')\n",
    "    print(self.q_table_final)\n",
    "    print()\n",
    "    print('Length of full Q-table =', len(self.q_table.index))\n",
    "    print('Full Q-table:')\n",
    "    print(self.q_table)\n",
    "    # Plotting the results for the number of steps\n",
    "def plot_results(self, steps, cost):\n",
    "    #\n",
    "    f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    #\n",
    "    ax1.plot(np.arange(len(steps)), steps, 'b')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Steps')\n",
    "    ax1.set_title('Episode via steps')\n",
    "    #\n",
    "    ax2.plot(np.arange(len(cost)), cost, 'r')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Cost')\n",
    "    ax2.set_title('Episode via cost')\n",
    "    # Showing the plots\n",
    "    plt.show()\n",
    "\n",
    "# Importing function from the env.py\n",
    "#from env import final_states\n",
    "# Creating class for the SarsaTable\n",
    "class SarsaTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        # List of actions\n",
    "        self.actions = actions\n",
    "        # Learning rate\n",
    "        self.lr = learning_rate\n",
    "        # Value of gamma\n",
    "        self.gamma = reward_decay\n",
    "        # Value of epsilon\n",
    "        self.epsilon = e_greedy\n",
    "        # Creating full Q-table for all cells\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "        # Creating Q-table for cells of the final route\n",
    "        self.q_table_final = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "    chs_act = choose_action\n",
    "    lrn = learn\n",
    "    ch_st_ext = check_state_exist\n",
    "    prt_q_tb = print_q_table\n",
    "    plt_rst = plot_results\n",
    "\n",
    "def update():\n",
    "    # Resulted list for the plotting Episodes via Steps\n",
    "    steps = []\n",
    "    # Summed costs for all episodes in resulted list\n",
    "    all_costs = []\n",
    "    for episode in range(1000):\n",
    "        # Initial Observation\n",
    "        observation = env.rst()\n",
    "        # Updating number of Steps for each Episode\n",
    "        i = 0\n",
    "        # Updating the cost for each episode\n",
    "        cost = 0\n",
    "        # RL choose action based on observation\n",
    "        action = RL.chs_act(str(observation))\n",
    "        while True:\n",
    "            # Refreshing environment\n",
    "            env.updt()\n",
    "            # RL takes an action and get the next observation = next state and reward\n",
    "            observation_, reward, done = env.action(action)\n",
    "            # RL choose action based on next observation\n",
    "            action_ = RL.chs_act(str(observation_))\n",
    "            # RL learns from the transition and calculating the cost\n",
    "            cost += RL.lrn(str(observation), action, reward, str(observation_), action_)\n",
    "            # Swapping the observations and actions - current and next\n",
    "            observation = observation_\n",
    "            action = action_\n",
    "            # Calculating number of Steps in the current Episode\n",
    "            i += 1\n",
    "            # Break while loop when it is the end of current Episode\n",
    "            # When agent reached the goal or obstacle\n",
    "            if done:\n",
    "                steps += [i]\n",
    "                all_costs += [cost]\n",
    "                break\n",
    "# Showing the final route\n",
    "env.dest()\n",
    "# Showing the Q-table with values for each action\n",
    "RL.prt_q_tb() \n",
    "# Plotting the results\n",
    "RL.plt_rst(steps, all_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to be implemented after running this file\n",
    "if __name__ == \"__main__\":\n",
    "    # Calling for the environment\n",
    "    env = Environment()\n",
    "    # Calling for the main algorithm\n",
    "    RL = SarsaTable(actions=list(range(env.n_actions)),\n",
    "    learning_rate=0.1,\n",
    "    reward_decay=0.9,\n",
    "    e_greedy=0.9)\n",
    "    # Running the main loop with Episodes by calling the function update()\n",
    "    env.after(100, update) # Or just update()\n",
    "    env.mainloop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a8ccd42af28b4c62894c86a0b2b7d79e16be9bf367118bf2f8f4681185a4a6f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
